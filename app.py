# ä»¥ä¸‹ã‚’ã€Œapp.pyã€ã«æ›¸ãè¾¼ã¿
import streamlit as st
from llama_index import VectorStoreIndex, ServiceContext, Document
from llama_index.llms import OpenAI
import openai
from llama_index import SimpleDirectoryReader

openai.api_key = st.secrets.OpenAIAPI.openai_api_key

st.header("Chat with the Streamlit docs ğŸ’¬ ğŸ“š")

if "messages" not in st.session_state.keys(): # Initialize the chat message history
    st.session_state.messages = [
        {"role": "assistant", "content": "Ask me a question about Streamlit's open-source Python library!"}
    ]

docs = SimpleDirectoryReader(input_dir="./data").load_data()
service_context = ServiceContext.from_defaults(llm=OpenAI(
  model="gpt-3.5-turbo",
  temperature=0.5,
  system_prompt="""
  {ãƒ†ãƒ¼ãƒ} = å®‰é”ã¨ã—ã¾ã‚€ã‚‰
  ã‚ãªãŸã¯{ãƒ†ãƒ¼ãƒ}ã®å°‚é–€å®¶ã§ã™ã€‚{ãƒ†ãƒ¼ãƒ}ã®ç†è§£ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã®4æŠå•é¡Œã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚ ã¯ã˜ã‚ã«åˆç´šãƒ¬ãƒ™ãƒ«ã®å•é¡Œã‹ã‚‰å§‹ã‚ã€æ­£ã—ã„å›ç­”ãŒå¾—ã‚‰ã‚Œã‚‹ãŸã³ã«å•é¡Œã®é›£æ˜“åº¦ã‚’å¾ã€…ã«ä¸Šã’ã¦ãã ã•ã„ã€‚

ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦æ±‚
ãƒ»ã‚ãªãŸãŒå‡ºé¡Œ
ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”
ãƒ»ã‚ãªãŸãŒæ­£è§£ã®ç™ºè¡¨
...
ä»¥ä¸‹åŒæ§˜ã«ç¹°ã‚Šè¿”ã™

ã¨ã„ã†æµã‚Œã§é€²ã¿ã¾ã™ã€‚ ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦æ±‚ã€ã¨ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã€ã®éƒ¨åˆ†ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã™ã‚‹éƒ¨åˆ†ã§ã™ã€‚ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å¾…ã¡ã¾ã™ã€‚

***ä»¥ä¸‹ã®æ‰‹é †ã«å³å¯†ã«å¾“ã£ã¦ãã ã•ã„***

###æ‰‹é †
1. å…¥åŠ›
2. å‡ºåŠ›
3. å¾…æ©Ÿãƒ¢ãƒ¼ãƒ‰
4. å…¥åŠ›'
5. å‡ºåŠ›'
6. æ‰‹é †1ã«æˆ»ã‚‹

ã€æ‰‹é †1ã€‘
!!! å¾…æ©Ÿãƒ¢ãƒ¼ãƒ‰: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ãŒã‚ã‚‹ã¾ã§å¾…æ©Ÿã—ã¾ã™ã€‚ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰è¦æ±‚ã®å…¥åŠ›ãŒã‚ã£ãŸã‚‰ã€ã€æ‰‹é †2ã€‘ã«é€²ã‚“ã§ãã ã•ã„ã€‚
<å…¥åŠ›>ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼š"ã‚¯ã‚¤ã‚ºå‡ºã—ã¦"

ã€æ‰‹é †2ã€‘
---å‡ºåŠ›æ§˜å¼---

{å•é¡Œæ–‡}
1. ï½›é¸æŠè‚¢1ï½
2. ï½›é¸æŠè‚¢2ï½
3. ï½›é¸æŠè‚¢3ï½
4. ï½›é¸æŠè‚¢4ï½


---å‡ºåŠ›æ§˜å¼ä»¥ä¸Š---
â€»å•é¡Œã¯1å•ã ã‘å‡ºã—ã¦ãã ã•ã„ã€‚
1å•å‡ºã—ãŸã‚‰ã€æ‰‹é †3ã€‘ã«é€²ã‚“ã§ãã ã•ã„ã€‚

ã€æ‰‹é †3ã€‘
!!! å¾…æ©Ÿãƒ¢ãƒ¼ãƒ‰: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ãŒã‚ã‚‹ã¾ã§å¾…æ©Ÿã—ã¾ã™ã€‚ ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç­”ãˆã‚’çµ¶å¯¾ã«å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰å›ç­”ã®å…¥åŠ›ãŒã‚ã£ãŸã‚‰ã€ã€æ‰‹é †4ã€‘ã«é€²ã‚“ã§ãã ã•ã„

ã€æ‰‹é †4ã€‘
<å…¥åŠ›>ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼š{å›ç­”}
â€»ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å›ç­”ãŒã‚ã£ãŸã‚‰ã€æ‰‹é †4ã€‘ã«é€²ã‚“ã§ãã ã•ã„

ã€æ‰‹é †5ã€‘
---å‡ºåŠ›æ§˜å¼---
{æ­£ã—ã„å›ç­”}{æ­£ã—ã„å›ç­”ã®è§£èª¬}

{å•é¡Œæ–‡}
1. ï½›é¸æŠè‚¢1ï½
2. ï½›é¸æŠè‚¢2ï½
3. ï½›é¸æŠè‚¢3ï½
4. ï½›é¸æŠè‚¢4ï½


---å‡ºåŠ›æ§˜å¼ä»¥ä¸Š---

ã€æ‰‹é †6ã€‘
 ã€æ‰‹é †1ã€‘ã«æˆ»ã£ã¦ãã ã•ã„
"""
))
index = VectorStoreIndex.from_documents(docs, service_context=service_context)

chat_engine = index.as_chat_engine(chat_mode="condense_question", verbose=True)

if prompt := st.text_input("Your question"): # Prompt for user input and save to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

for message in st.session_state.messages: # Display the prior chat messages
    speaker = "ğŸ™‚"
    if message["role"]=="assistant":
        speaker="ğŸ¤–"
        
        st.write(speaker + ": " + message["content"])


# If last message is not from assistant, generate a new response
if st.session_state.messages[-1]["role"] != "assistant":
        with st.spinner("Thinking..."):
            response = chat_engine.chat(prompt)
            st.write(response.response)
            message = {"role": "assistant", "content": response.response}
            st.session_state.messages.append(message) # Add response to message history
