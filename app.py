import streamlit as st
from llama_index import VectorStoreIndex, ServiceContext, Document
from llama_index.llms import OpenAI
import openai
from llama_index import SimpleDirectoryReader

st.set_page_config(page_title="Chat with the Streamlit docs, powered by LlamaIndex", page_icon="ğŸ¦™", layout="centered", initial_sidebar_state="auto", menu_items=None)
openai.api_key = st.secrets.OpenAIAPI.openai_api_key
st.title("æ•™å¸«ChatBotã‚¢ãƒ—ãƒª")
         
if "messages" not in st.session_state.keys(): # Initialize the chat messages history
    st.session_state.messages = [
        {"role": "assistant", "content": "ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯è³ªå•ã«å¯¾ã—ã¦ã€è§£èª¬ã¨ç¢ºèªã‚¯ã‚¤ã‚ºã‚’å‡ºã™ChatBotã§ã™ã€‚ä½•ã§ã‚‚è³ªå•ã—ã¦ãã ã•ã„ï¼"}
    ]
         

@st.cache_resource(show_spinner=False)
# ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¨ã‚„ã‚Šã¨ã‚Šã™ã‚‹é–¢æ•°
def load_data():
    with st.spinner(text="ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„"):
        reader = SimpleDirectoryReader(input_dir="./data", recursive=True)
        docs = reader.load_data()
        service_context = ServiceContext.from_defaults(llm=OpenAI(model="gpt-3.5-turbo", temperature=0.5, system_prompt="""
        {ãƒ†ãƒ¼ãƒ} = ã€Œå®‰é”ã¨ã—ã¾ã‚€ã‚‰ã€ã¨ã€Œç¾ä»£å“²å­¦ã€ 
        
        ã‚ãªãŸã¯{ãƒ†ãƒ¼ãƒ}ã®å°‚é–€å®¶ã§ã™ã€‚ã‚ãªãŸã¯[å…¥åŠ›è€…]ã«å¯¾ã—ã€ä»¥ä¸‹ã®æ‰‹é †ã§{ãƒ†ãƒ¼ãƒ}ã«å¯¾ã™ã‚‹ç†è§£ã‚’æ·±ã‚ã•ã›ã¦ãã ã•ã„ã€‚
        æ‰‹é †ï¼‘ï¼š[å…¥åŠ›è€…]ã®å…¥åŠ›ã‚’å¾…ã¤
        æ‰‹é †ï¼’ï¼š{ãƒ†ãƒ¼ãƒ}ã®å…¥åŠ›ã«å¯¾ã—ã¦è©³ç´°ãªèª¬æ˜ã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚ 
        æ‰‹é †ï¼“ï¼šæ‰‹é †ï¼’ã§èª¬æ˜ã—ãŸå†…å®¹ã«ã¾ã¤ã‚ã‚‹è³ªå•ã‚’ï¼”æŠã§ã—ã¦ãã ã•ã„ã€‚
        æ‰‹é †ï¼”ï¼š[å…¥åŠ›è€…]ã®å…¥åŠ›ã‚’å¾…ã¤ã€‚
        æ‰‹é †ï¼•ï¼šæ‰‹é †ï¼”ã§æ‰‹é †ï¼“ã§å‡ºã—ãŸè³ªå•ã«å¯¾ã™ã‚‹å›ç­”ï¼ˆa~dã®ã„ãšã‚Œã‹ï¼‰ã‚’å—ã‘å–ã£ãŸå ´åˆã¯ã€æ­£èª¤ã¨æ¨¡ç¯„è§£ç­”ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚
        æ‰‹é †ï¼–ï¼šæ‰‹é †ï¼‘ã«æˆ»ã‚‹ã€‚

        â€»ä»¥ä¸‹æ‰‹é †ï¼“ã®å‡ºåŠ›å½¢å¼
        Q.[è³ªå•å†…å®¹]

        a.[é¸æŠè‚¢ï¼‘]
        b.[é¸æŠè‚¢ï¼’]
        c.[é¸æŠè‚¢ï¼“]
        d.[é¸æŠè‚¢ï¼”]
        â€»æ‰‹é †ï¼“ã®å‡ºåŠ›å½¢å¼ã“ã“ã¾ã§

        â€»ä»¥ä¸‹æ‰‹é †ï¼•ã®å‡ºåŠ›å½¢å¼
        ã€Œæ­£è§£ã§ã™ï¼ã€or ã€Œä¸æ­£è§£ã§ã™ã€‚ã€
        A.[æ¨¡ç¯„è§£ç­”]
        â€»æ‰‹é †ï¼•ã®å‡ºåŠ›å½¢å¼ã“ã“ã¾ã§
        """))
        index = VectorStoreIndex.from_documents(docs, service_context=service_context)
        return index


index = load_data()


if "chat_engine" not in st.session_state.keys(): # Initialize the chat engine
        st.session_state.chat_engine = index.as_chat_engine(chat_mode="condense_question", verbose=True)

if prompt := st.chat_input("Your question"): # Prompt for user input and save to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

for message in st.session_state.messages: # Display the prior chat messages
    with st.chat_message(message["role"]):
        st.write(message["content"])

# If last message is not from assistant, generate a new response
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("è€ƒãˆä¸­..."):
            response = st.session_state.chat_engine.chat(prompt)
            st.write(response.response)
            message = {"role": "assistant", "content": response.response}
            st.session_state.messages.append(message) # Add response to message history
